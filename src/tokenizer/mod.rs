//! xml tokenizer
use std::collections::HashMap;
use std::borrow::Cow::{self, Borrowed};

use tendril::StrTendril;
mod states;

use super::utils::{SmallCharSet};
mod interface;
mod input;


use states::States;
pub use interface::{ 
    Sink, 
    TokenKind, 
    TokenName,
    Token,
    SinkResult,
    RawToken
};
use input:: {SetResult, TokenizerInput};
use SetResult::{FromSet, NotFromSet};
use TokenKind::{Opening, Closing};

/// breaks a given input into a linear stream of token
/// 
/// Tokens generated by the tokenizer are fed to the sink for processing,
/// the tokenizer keeps track of how much time is spent in the sink in nano seconds and reports them to the user on demand
pub struct Tokenizer<TokenSink: Sink> {
    state: States,                   // keeps track of the current tokenizer state
    current_line: u32,              // tracks the current line 
    current_char: char,             // tracks the current processing character  
    current_token_self_closing: bool,        // does the current token self close
    sink: TokenSink,                         // processes emitted tokens
    reconsume: bool,                    // reconsume this character
    emitted_characters: StrTendril,             // stores the characters not from set 
    current_token_name: StrTendril,             // name of the current processing 
    current_token_attributes: HashMap<StrTendril, StrTendril>,          // track the attributes in the current tag
    current_token_kind: TokenKind,
    current_token_attr_name: StrTendril,                            // name of currently processing attribute
    current_token_attr_value: StrTendril,                           // value of the currently processing atrtibute
    last_opening_token: Option<TokenName>,                                     // name of the last start token, verify closing tokens \
    input_buffer: TokenizerInput 
}

impl <TokenSink:Sink> Tokenizer<TokenSink> {
    /// create a new tokenizer instance
    pub fn new(sink: TokenSink) -> Self {
        Self {
            state: States::default(),
            current_line: 0,
            current_char: '\0',
            current_token_self_closing: false,
            sink,
            emitted_characters: StrTendril::new(),
            current_token_name: StrTendril::new(),
            current_token_attributes: HashMap::new(),
            current_token_attr_name: StrTendril::new(),
            current_token_attr_value: StrTendril::new(),
            last_opening_token: None,
            reconsume: false,
            current_token_kind: TokenKind::Opening,
            input_buffer: TokenizerInput::new()
        }
    }


    pub fn sink(&self) -> &TokenSink {
        &self.sink
    }

    /// Feed the input to the tokenizer state machine untill we have no more input
    /// 
    pub fn feed(&mut self, input: StrTendril) -> bool {
        if input.is_empty(){
            return false;
        } 

        self.input_buffer.push_back(input);

        self.run();

        true 
    }

    /// Check if we have any buffers remaining for processing
    pub fn is_empty(&self) -> bool {
        self.input_buffer.is_empty()
    }

    /// Get the length of remaining buffers
    pub fn len(&self) -> usize {
        self.input_buffer.len()
    }

    /// finish processing all impending buffers
    pub fn end(&mut self){
        // wait for all buffers to complete processing 
        loop {
            if self.is_empty(){
                break;
            }else{
                // process any impending buffer 
                self.step();
            }
        }

        self.sink.end()
    }

    /// Process the tokens untill we have run out 
    fn run(&mut self) {
        while self.step(){}
    }

    /// emit given character
    fn emit_characters(&mut self, characters: StrTendril){
        // append this string to the current string
        self.emitted_characters.push_tendril(&characters)

    }

    /// create a token to emit 
    /// Called when we encounter an opening token '<' then the token name NotFromSet(name)
    fn create_token(&mut self, token_kind: TokenKind, name: StrTendril) {
        // discard the previous token
        self.discard_token();

        self.current_token_name = name;
        self.current_token_kind = token_kind;
        self.current_token_attributes = HashMap::new()
    }


    /// Discard the previous tag ready to build a new tag 
    fn discard_token(&mut self){
        // clean the token data 
        self.current_token_name.clear();
        self.current_token_self_closing = false;
        self.current_token_kind = TokenKind::Opening;
        self.current_token_attributes.clear()
    }


    /// Pop and process a character within a given set of characters from the given tokenizer input
    fn pop_except_from(&mut self, set: SmallCharSet) -> Option<SetResult> {
        // should we reconsume this character
        if self.reconsume {
            return self.get_char().map(|x| FromSet(x));
        }

        let d = self.input_buffer.pop_except_from(set);

        match d {
            Some(FromSet(c)) => self.get_preprocessed_char(c).map(|x| FromSet(x)),
            _ => d 
        }

    }

    /// Retrieve a character from the input passed
    /// If reconsume flag is set, return the current character
    fn get_char(&mut self) -> Option<char> {
        if self.reconsume {
            self.reconsume = false;
            Some(self.current_char)
        }else {
            self.input_buffer.next()
                .and_then(|c| self.get_preprocessed_char(c))
        }
    }

    /// Get the next input character
    /// Takes care of new lines and invalid characters
    fn get_preprocessed_char(&mut self, c: char) -> Option<char>{
        // handle new lines
        if c == '\n'{
            self.current_line += 1;
        }

        self.current_char = c;

        Some(c)
    }

    /// Emit a token name 
    fn emit_token_name(&mut self, token_name: StrTendril) {
        self.current_token_name = token_name;
    }

    /// Emit a token to the sink 
    /// 
    /// Creates a new token using registered token data in the state
    fn emit_token(&mut self){
        // finish the currently processing token attributes
        self.finish_attribute();

        let token_name = TokenName(self.current_token_name.clone());
        // clear the token name 
        self.current_token_name.clear();

        //process by tag kind 
        match self.current_token_kind {
            TokenKind::Opening => {
                self.last_opening_token = Some(token_name.clone())
            },
            TokenKind::Closing => {
                // check that there are no attributes on the closing tag 
                if !self.current_token_attributes.is_empty(){
                    self.emit_error(Borrowed("Should not have attributes on a closing token"))
                }

                if self.current_token_self_closing{
                    self.emit_error(Borrowed("Invalid self closing tag on a closing tag"))
                }
            }

            
        };
        // get the character to be attached as value to the token 
        let characters = if self.emitted_characters.is_empty(){
            None 
        }else{
            Some(self.emitted_characters.clone())
        };


        // clear the characters 
        self.emitted_characters.clear();


        let token = RawToken {
            name: token_name,
            kind: self.current_token_kind,
            attributes: self.current_token_attributes.clone(),
            self_closing: self.current_token_self_closing,
            value: characters

        };

        // reset self closing
        self.current_token_self_closing = false;

        // emit to sink 
        self.process_and_continue(Token::Token(token))
    }

    /// Commit a currently processing attribute name, value key
    fn finish_attribute(&mut self){
        if self.current_token_attr_name.is_empty(){
            return 
        }

        // commit the attribute 
        self.emit_attributes();
    }

    /// Create an attribute from current attr name and value set 
    fn emit_attributes(&mut self){
        self.current_token_attributes.insert(self.current_token_attr_name.clone(),self.current_token_attr_value.clone());

        // clear the attrs 
        self.current_token_attr_name.clear();
        self.current_token_attr_value.clear();
    }

    /// throw an error to the sink 
    fn emit_error(&mut self, error: Cow<'static, str>){
        // create an error 
        let error_token = Token::Error(error);

        self.process_and_continue(error_token)
    }

    /// Process a token and continue 
    fn process_and_continue(&mut self, token: Token){
        assert_eq!(
            self.process_token(token),
            SinkResult::Continue
        );
    }

    /// emit a token to the registered sink 
    fn process_token(&mut self, token: Token) -> SinkResult{
        self.sink.process(token, self.current_line)
    }

    /// set the current token to self close 
    fn emit_self_closing(&mut self) {
        self.current_token_self_closing = true;
    }

    /// emit an attrbute name 
    fn emit_attribute_name(&mut self, attr_name: StrTendril ){
        self.current_token_attr_name = attr_name;
    }

    /// Emits when a bad character is encountered during tokenzation 
    fn bad_char_error(&mut self){
        let msg = format!(
            "Bad character seen {} in state {:?} on line {}",
            self.current_char,
            self.state,
            self.current_line
        );

        self.emit_error(Cow::from(msg));
    }

    /// emit a given attribute value 
    fn emit_attribute_value(&mut self, attr_val: StrTendril){
        self.current_token_attr_value = attr_val;
    }


} 



/// Macros used by the tokenizer
macro_rules! pop_from_set(
    (  $me:expr, $set:expr) => (
        unwrap_or_return!($me.pop_except_from($set), false)
    )
);

/// Short hand macros for commond operations
macro_rules! shorthand (
    (  $me:ident    :   emit_characters    $c:ident     ) =>   ( $me.emit_characters($c);         );
    (  $me:ident    :   emit_token                      ) =>   ( $me.emit_token();                );
    (  $me:ident    :   emit_token_name   $name:ident   )  =>  ( $me.emit_token_name($name);      );
    (  $me:ident    :   emit_self_closing               )  =>  ( $me.emit_self_closing();          );
    (  $me:ident    :   emit_token   $kind:ident  $name:ident   )      =>     ( $me.create_token($kind, $name)            );
    (  $me:ident    :   emit_attribute                  )       => ( $me.emit_attributes();                                      );
    // emit a bad_char error 
    (  $me:ident    : error                                     )      =>     (  $me.bad_char_error();                    );
);

/// A little DSl for our state machine 
macro_rules! go(
    // methods to call methods in self
    (  $me:ident : $a:tt                            ; $($rest:tt)*  ) => ({  shorthand!($me: $a);              go!($me: $($rest)*);     });
    (  $me:ident : $a:tt $b:tt                      ; $($rest:tt)*  ) => ({  shorthand!($me: $a $b);              go!($me: $($rest)*);     });
    (  $me:ident : $a:tt $b:tt  $c:tt               ; $($rest:tt)*  ) => ({ shorthand!($me: $a $b $c);        go!($me: $($rest)*);      });

    // macros to advance the state
    (  $me:ident :     to $s:ident                                  ) => ({ $me.state = States::$s; return false; });
);

impl<TokenSink: Sink> Tokenizer<TokenSink>{
    /// Run the tokenizer state machine as long as we can
    fn step(&mut self) -> bool {
        // println!("processing in state {:?}", self.state);

        match self.state {
            States::Document => loop {
                let set = small_char_set!(b'<' b' ' b'\n' b'\t' b'>' b'\0' b'?');

                // read a character from the token set 
                match pop_from_set!(self, set){
                    FromSet('<') => go!(self: to OpeningToken),
                    FromSet(' ' ) | FromSet('\n') | FromSet('\t') => (),     // ignore white space characters
                    FromSet('>') => go!(self: to ClosingToken),
                    FromSet('\0') => return false,
                    FromSet('?') => go!(self: to ProcessingInstruction),
                    // to prevent `non-exhaustive patterns` complain by the compiler
                    FromSet(_) => (),
                    NotFromSet(c) => go!(self: emit_characters c; to Characters),
                }
            },

            // on opening token
            States::OpeningToken => loop {
                let set = small_char_set!(b'?' b'/' b'>' b' ');

                match pop_from_set!(self, set) {
                    FromSet('/') => go!(self: to ClosingToken),
                    FromSet('>') => go!(self: emit_token; to Document),
                    FromSet(' ') => go!(self: to BeforeAttributeName),
                    FromSet('?') => go!(self: to ProcessingInstruction),
                    // to prevent `non-exhaustive patterns` complain by the compiler
                    FromSet(_) => (),
                    NotFromSet(c) => self.create_token(Opening, c)
                }
            } ,

            // a closing token 
            States::ClosingToken => loop {
                let set = small_char_set!(b'>');

                match pop_from_set!(self, set){
                    FromSet('>') => go!(self: emit_self_closing; to Document),
                    // to prevent `non-exhaustive patterns` complain by the compiler
                    FromSet(_) => (),
                    NotFromSet(c) => go!(self: emit_token Closing c; to TokenName ),
                }
            },

            // before processing an attribute name
            States::BeforeAttributeName => loop {
                let set = small_char_set!(b'=' b'/' b'>' b'?' b' ');

                match pop_from_set!(self, set){
                    FromSet('=') => go!(self: to BeforeAttributeValue),
                    FromSet('/') => go!(self: to ClosingToken),
                    FromSet('>') => go!(self: emit_token; to Document),
                    FromSet('?') => go!(self: to ProcessingInstruction),
                    // to prevent `non-exhaustive patterns` complain by the compiler
                    FromSet(_) => (),
                    NotFromSet(c) => self.emit_attribute_name(c)
                }
            },

            // Before processing an attribute name 
            States::BeforeAttributeValue => loop {
                match self.input_buffer.next(){
                    Some('"') | Some('\'') => go!(self: to AttributeValue),
                    _ => go!(self: error; to Document)
                }
            },

            // Processing instruction 
            States::ProcessingInstruction => loop {
                let set = small_char_set!(b'>' b' ');

                match pop_from_set!(self, set){
                    FromSet('>') => go!(self: emit_token; to Document ),
                    FromSet(' ') => go!(self: to BeforeAttributeName),
                    // to prevent `non-exhaustive patterns` complain by the compiler
                    FromSet(_) => (),
                    NotFromSet(c) => go!(self: emit_token_name c; to TokenName),
                }
            },

            // emitted characters
            States::Characters => loop {
                let set = small_char_set!(b'<');

                match pop_from_set!(self, set){
                    FromSet('<') =>  go!(self: to OpeningToken),
                    // to prevent `non-exhaustive patterns` complain by the compiler
                    FromSet(_) => (),
                    NotFromSet(c) => {
                        self.emit_characters(c);
                        return false
                    },
                }
            },

            // processing a token name 
            States::TokenName => loop {
                let set = small_char_set!(b'/' b' ' b'>' b'\n' b'\t');

                match pop_from_set!(self, set){
                    FromSet('/') => self.current_token_self_closing = true,
                    FromSet('>') => go!(self: emit_token; to Document),
                    FromSet(' ') | FromSet('\n') | FromSet('\t') => go!(self: to BeforeAttributeName),
                     // to prevent `non-exhaustive patterns` complain by the compiler
                     _ => (),
                }
            },

            // processing an attribute value 
            States::AttributeValue => loop {
                let set = small_char_set!(b'"'  b'\'' b' ');

                match pop_from_set!(self, set){
                    FromSet('"') | FromSet('\'') => go!(self: emit_attribute; to BeforeAttributeName),
                    FromSet(' ') => go!(self: to BeforeAttributeName),
                    // to prevent `non-exhaustive patterns` complain by the compiler
                    FromSet(_) => (),
                    NotFromSet(c) => self.emit_attribute_value(c),
                }
            }
        }
    }
}
